{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2386b23",
   "metadata": {},
   "source": [
    "## CIFAR-10 이미지 추출\n",
    "### 클라이언트별 IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be5487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 11:54:39.482321: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-04 11:54:39.482340: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e6e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "images = np.concatenate((train_images, test_images))\n",
    "labels = np.concatenate((train_labels, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c851454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data\n",
    "labelnames = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "labelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00b6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = 10\n",
    "div = clients + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479a411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {}\n",
    "output = os.path.abspath(os.path.expanduser('dataset'))\n",
    "for idx, data in enumerate(zip(images, labels), start=0):\n",
    "    image = Image.fromarray(data[0])\n",
    "    label = labelnames[data[1]]\n",
    "    num = counter.get(label, 0)\n",
    "    party = num%div\n",
    "    if party != 0:\n",
    "        party = random.randint(1, clients)\n",
    "    odir = os.path.join(output, f'{party}', label)\n",
    "    os.makedirs(odir, exist_ok=True)\n",
    "    opath = os.path.join(odir, f'{num:04d}.jpg')\n",
    "    image.save(opath)\n",
    "    counter[label] = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f15db",
   "metadata": {},
   "source": [
    "## 클라이언트별 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083cc00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/2 2\n",
      "<DirEntry '9'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/9 9\n",
      "<DirEntry '3'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/3 3\n",
      "<DirEntry '7'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/7 7\n",
      "<DirEntry '10'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/10 10\n",
      "<DirEntry '6'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/6 6\n",
      "<DirEntry '1'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/1 1\n",
      "<DirEntry '4'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/4 4\n",
      "<DirEntry '8'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/8 8\n",
      "<DirEntry '5'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/5 5\n",
      "<DirEntry '0'> /home/harny/Github/flhub-experiments/KCC2022/MNISTFashion/EvenOdd/dataset/0 0\n"
     ]
    }
   ],
   "source": [
    "dataset_root = os.path.abspath(os.path.expanduser('dataset'))\n",
    "with os.scandir(dataset_root) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_dir():\n",
    "            print(entry, entry.path, entry.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bf6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5123 images belonging to 10 classes.\n",
      "Found 1275 images belonging to 10 classes.\n",
      "Found 5013 images belonging to 10 classes.\n",
      "Found 1247 images belonging to 10 classes.\n",
      "Found 5070 images belonging to 10 classes.\n",
      "Found 1262 images belonging to 10 classes.\n",
      "Found 5099 images belonging to 10 classes.\n",
      "Found 1269 images belonging to 10 classes.\n",
      "Found 5073 images belonging to 10 classes.\n",
      "Found 1264 images belonging to 10 classes.\n",
      "Found 5177 images belonging to 10 classes.\n",
      "Found 1289 images belonging to 10 classes.\n",
      "Found 5079 images belonging to 10 classes.\n",
      "Found 1264 images belonging to 10 classes.\n",
      "Found 5062 images belonging to 10 classes.\n",
      "Found 1262 images belonging to 10 classes.\n",
      "Found 5156 images belonging to 10 classes.\n",
      "Found 1283 images belonging to 10 classes.\n",
      "Found 5093 images belonging to 10 classes.\n",
      "Found 1270 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['2', '9', '3', '7', '10', '6', '1', '4', '8', '5'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets\n",
    "datasets = {}\n",
    "dataset_root = os.path.abspath(os.path.expanduser('dataset'))\n",
    "with os.scandir(dataset_root) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_dir():\n",
    "            if entry.name == '0':\n",
    "                continue\n",
    "            image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255, validation_split=0.2)\n",
    "            train = image_generator.flow_from_directory(entry.path,\n",
    "                                                        classes=labelnames,\n",
    "                                                        target_size=(32, 32),\n",
    "                                                        subset='training',\n",
    "                                                        shuffle=True)\n",
    "            test = image_generator.flow_from_directory(entry.path,\n",
    "                                                       classes=labelnames,\n",
    "                                                       target_size=(32, 32),\n",
    "                                                       subset='validation',\n",
    "                                                       shuffle=True)\n",
    "            datasets[entry.name] = (train, test)\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d74bc5d-4647-44d6-9e07-0d02623c170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6370 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['2', '9', '3', '7', '10', '6', '1', '4', '8', '5', '0'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "datasets['0'] = image_generator.flow_from_directory(os.path.join(dataset_root, '0'),\n",
    "                                                    classes=labelnames, \n",
    "                                                    target_size=(32, 32), \n",
    "                                                    shuffle=True)\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c51529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 11:54:47.033749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-04 11:54:47.033776: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-04 11:54:47.033788: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LuHa-X1C9): /proc/driver/nvidia/version does not exist\n",
      "2022-05-04 11:54:47.033941: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "                       tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "                       tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                       tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                       tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                       tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                       tf.keras.layers.Flatten(),\n",
    "                       tf.keras.layers.Dense(64, activation='relu'),\n",
    "                       tf.keras.layers.Dense(len(labelnames))])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35367cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 11:54:47.265362: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/0/0/assets\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "odir = os.path.join('models', '0', '0')\n",
    "model.save(odir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fe98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02074a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/0/2/assets\n",
      "Local train: round #0 with clinent #2\n",
      "INFO:tensorflow:Assets written to: models/0/4/assets\n",
      "Local train: round #0 with clinent #4\n",
      "INFO:tensorflow:Assets written to: models/0/6/assets\n",
      "Local train: round #0 with clinent #6\n",
      "INFO:tensorflow:Assets written to: models/0/8/assets\n",
      "Local train: round #0 with clinent #8\n",
      "INFO:tensorflow:Assets written to: models/0/10/assets\n",
      "Local train: round #0 with clinent #10\n",
      "INFO:tensorflow:Assets written to: models/1/0/assets\n",
      "Global aggregation: round #1 for [0.5130490064620972, 0.8401883840560913]\n",
      "INFO:tensorflow:Assets written to: models/1/1/assets\n",
      "Local train: round #1 with clinent #1\n",
      "INFO:tensorflow:Assets written to: models/1/3/assets\n",
      "Local train: round #1 with clinent #3\n",
      "INFO:tensorflow:Assets written to: models/1/5/assets\n",
      "Local train: round #1 with clinent #5\n",
      "INFO:tensorflow:Assets written to: models/1/7/assets\n",
      "Local train: round #1 with clinent #7\n",
      "INFO:tensorflow:Assets written to: models/1/9/assets\n",
      "Local train: round #1 with clinent #9\n",
      "INFO:tensorflow:Assets written to: models/2/0/assets\n",
      "Global aggregation: round #2 for [0.3803803324699402, 0.8817896246910095]\n",
      "INFO:tensorflow:Assets written to: models/2/2/assets\n",
      "Local train: round #2 with clinent #2\n",
      "INFO:tensorflow:Assets written to: models/2/4/assets\n",
      "Local train: round #2 with clinent #4\n",
      "INFO:tensorflow:Assets written to: models/2/6/assets\n",
      "Local train: round #2 with clinent #6\n",
      "INFO:tensorflow:Assets written to: models/2/8/assets\n",
      "Local train: round #2 with clinent #8\n",
      "INFO:tensorflow:Assets written to: models/2/10/assets\n",
      "Local train: round #2 with clinent #10\n",
      "INFO:tensorflow:Assets written to: models/3/0/assets\n",
      "Global aggregation: round #3 for [0.3924536108970642, 0.8883830308914185]\n",
      "INFO:tensorflow:Assets written to: models/3/1/assets\n",
      "Local train: round #3 with clinent #1\n",
      "INFO:tensorflow:Assets written to: models/3/3/assets\n",
      "Local train: round #3 with clinent #3\n",
      "INFO:tensorflow:Assets written to: models/3/5/assets\n",
      "Local train: round #3 with clinent #5\n",
      "INFO:tensorflow:Assets written to: models/3/7/assets\n",
      "Local train: round #3 with clinent #7\n",
      "INFO:tensorflow:Assets written to: models/3/9/assets\n",
      "Local train: round #3 with clinent #9\n",
      "INFO:tensorflow:Assets written to: models/4/0/assets\n",
      "Global aggregation: round #4 for [0.4238416254520416, 0.8929356336593628]\n",
      "INFO:tensorflow:Assets written to: models/4/2/assets\n",
      "Local train: round #4 with clinent #2\n",
      "INFO:tensorflow:Assets written to: models/4/4/assets\n",
      "Local train: round #4 with clinent #4\n",
      "INFO:tensorflow:Assets written to: models/4/6/assets\n",
      "Local train: round #4 with clinent #6\n",
      "INFO:tensorflow:Assets written to: models/4/8/assets\n",
      "Local train: round #4 with clinent #8\n",
      "INFO:tensorflow:Assets written to: models/4/10/assets\n",
      "Local train: round #4 with clinent #10\n",
      "INFO:tensorflow:Assets written to: models/5/0/assets\n",
      "Global aggregation: round #5 for [0.47575515508651733, 0.8949764370918274]\n",
      "INFO:tensorflow:Assets written to: models/5/1/assets\n",
      "Local train: round #5 with clinent #1\n",
      "INFO:tensorflow:Assets written to: models/5/3/assets\n",
      "Local train: round #5 with clinent #3\n",
      "INFO:tensorflow:Assets written to: models/5/5/assets\n",
      "Local train: round #5 with clinent #5\n",
      "INFO:tensorflow:Assets written to: models/5/7/assets\n",
      "Local train: round #5 with clinent #7\n",
      "INFO:tensorflow:Assets written to: models/5/9/assets\n",
      "Local train: round #5 with clinent #9\n",
      "INFO:tensorflow:Assets written to: models/6/0/assets\n",
      "Global aggregation: round #6 for [0.4839341640472412, 0.8952904343605042]\n",
      "INFO:tensorflow:Assets written to: models/6/2/assets\n",
      "Local train: round #6 with clinent #2\n",
      "INFO:tensorflow:Assets written to: models/6/4/assets\n",
      "Local train: round #6 with clinent #4\n",
      "INFO:tensorflow:Assets written to: models/6/6/assets\n",
      "Local train: round #6 with clinent #6\n",
      "INFO:tensorflow:Assets written to: models/6/8/assets\n",
      "Local train: round #6 with clinent #8\n",
      "INFO:tensorflow:Assets written to: models/6/10/assets\n",
      "Local train: round #6 with clinent #10\n",
      "INFO:tensorflow:Assets written to: models/7/0/assets\n",
      "Global aggregation: round #7 for [0.4798673689365387, 0.8979591727256775]\n",
      "INFO:tensorflow:Assets written to: models/7/1/assets\n",
      "Local train: round #7 with clinent #1\n",
      "INFO:tensorflow:Assets written to: models/7/3/assets\n",
      "Local train: round #7 with clinent #3\n",
      "INFO:tensorflow:Assets written to: models/7/5/assets\n",
      "Local train: round #7 with clinent #5\n",
      "INFO:tensorflow:Assets written to: models/7/7/assets\n",
      "Local train: round #7 with clinent #7\n",
      "INFO:tensorflow:Assets written to: models/7/9/assets\n",
      "Local train: round #7 with clinent #9\n",
      "INFO:tensorflow:Assets written to: models/8/0/assets\n",
      "Global aggregation: round #8 for [0.5189032554626465, 0.9003139734268188]\n",
      "INFO:tensorflow:Assets written to: models/8/2/assets\n",
      "Local train: round #8 with clinent #2\n",
      "INFO:tensorflow:Assets written to: models/8/4/assets\n",
      "Local train: round #8 with clinent #4\n",
      "INFO:tensorflow:Assets written to: models/8/6/assets\n",
      "Local train: round #8 with clinent #6\n",
      "INFO:tensorflow:Assets written to: models/8/8/assets\n",
      "Local train: round #8 with clinent #8\n",
      "INFO:tensorflow:Assets written to: models/8/10/assets\n",
      "Local train: round #8 with clinent #10\n",
      "INFO:tensorflow:Assets written to: models/9/0/assets\n",
      "Global aggregation: round #9 for [0.5112210512161255, 0.897017240524292]\n",
      "INFO:tensorflow:Assets written to: models/9/1/assets\n",
      "Local train: round #9 with clinent #1\n",
      "INFO:tensorflow:Assets written to: models/9/3/assets\n",
      "Local train: round #9 with clinent #3\n",
      "INFO:tensorflow:Assets written to: models/9/5/assets\n",
      "Local train: round #9 with clinent #5\n",
      "INFO:tensorflow:Assets written to: models/9/7/assets\n",
      "Local train: round #9 with clinent #7\n",
      "INFO:tensorflow:Assets written to: models/9/9/assets\n",
      "Local train: round #9 with clinent #9\n",
      "INFO:tensorflow:Assets written to: models/10/0/assets\n",
      "Global aggregation: round #10 for [0.5222451686859131, 0.8949764370918274]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for r in range(rounds):\n",
    "    part = 0\n",
    "    weights = []\n",
    "    rpath = os.path.join('models', f'{r}')\n",
    "    for c in range(1, clients+1):\n",
    "        if r%2 != c%2:\n",
    "            continue\n",
    "        mpath = os.path.join(rpath, '0')\n",
    "        model = tf.keras.models.load_model(mpath)\n",
    "        history = model.fit(datasets[f'{c}'][0], epochs=epochs, verbose=0)\n",
    "        opath = os.path.join(rpath, f'{c}')\n",
    "        model.save(opath)\n",
    "        if len(weights) == 0:\n",
    "            weights = model.get_weights()\n",
    "            part = part + 1\n",
    "        else:\n",
    "            for idx, weight in enumerate(model.get_weights()):\n",
    "                weights[idx] = weights[idx] + weight\n",
    "            part = part + 1\n",
    "        print(f'Local train: round #{r} with clinent #{c}')\n",
    "    for idx, weight in enumerate(weights):\n",
    "        weights[idx] = weights[idx] / part\n",
    "    mpath = os.path.join(rpath, '0')\n",
    "    model = tf.keras.models.load_model(mpath)\n",
    "    model.set_weights(weights)\n",
    "    ndir = os.path.join('models', f'{r+1}')\n",
    "    npath = os.path.join(ndir, '0')\n",
    "    model.save(npath)\n",
    "    metric = model.evaluate(datasets['0'], verbose=0)\n",
    "    print(f'Global aggregation: round #{r+1} for {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c977bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round #1 - Global: [0.5130488872528076, 0.8401883840560913]\n",
      "round #1 - Federated: (0.48167566657066346, 0.8530444502830505)\n",
      "round #2 - Global: [0.380380243062973, 0.8817896246910095]\n",
      "round #2 - Federated: (0.2848794341087341, 0.9000699520111084)\n",
      "round #3 - Global: [0.39245352149009705, 0.8883830308914185]\n",
      "round #3 - Federated: (0.24798900187015532, 0.9181190013885498)\n",
      "round #4 - Global: [0.423841655254364, 0.8929356336593628]\n",
      "round #4 - Federated: (0.2225220739841461, 0.9263599634170532)\n",
      "round #5 - Global: [0.47575488686561584, 0.8949764370918274]\n",
      "round #5 - Federated: (0.21194939017295839, 0.9363281965255738)\n",
      "round #6 - Global: [0.48393383622169495, 0.8952904343605042]\n",
      "round #6 - Federated: (0.193728768825531, 0.9390581965446472)\n",
      "round #7 - Global: [0.4798671007156372, 0.8979591727256775]\n",
      "round #7 - Federated: (0.17403753995895385, 0.945078432559967)\n",
      "round #8 - Global: [0.5189034938812256, 0.9003139734268188]\n",
      "round #8 - Federated: (0.15835869908332825, 0.9507721304893494)\n",
      "round #9 - Global: [0.5112208127975464, 0.897017240524292]\n",
      "round #9 - Federated: (0.13914150297641753, 0.9557788968086243)\n"
     ]
    }
   ],
   "source": [
    "global_acc = []\n",
    "global_loss = []\n",
    "federated_acc = []\n",
    "federated_loss = []\n",
    "for r in range(1, rounds):\n",
    "    part = 0\n",
    "    weights = []\n",
    "    mpath = os.path.join('models', f'{r}', '0')\n",
    "    model = tf.keras.models.load_model(mpath)\n",
    "    acc = []\n",
    "    loss = []\n",
    "    for c in range(0, clients+1):\n",
    "        if c == 0:\n",
    "            metric = model.evaluate(datasets[f'{c}'], verbose=0)\n",
    "            global_loss.append(metric[0])\n",
    "            global_acc.append(metric[1])\n",
    "            print(f'round #{r} - Global: {metric}')\n",
    "        else:\n",
    "            if r%2 == c%2:\n",
    "                continue\n",
    "            metric = model.evaluate(datasets[f'{c}'][0], verbose=0)\n",
    "            loss.append(metric[0])\n",
    "            acc.append(metric[1])\n",
    "    loss = sum(loss) / len(loss)\n",
    "    acc = sum(acc) / len(acc)\n",
    "    federated_loss.append(loss)\n",
    "    federated_acc.append(acc)\n",
    "    print(f'round #{r} - Federated: {(loss, acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb6eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_acc = []\n",
    "global_loss = []\n",
    "federated_acc = []\n",
    "federated_loss = []\n",
    "for r in range(1, rounds):\n",
    "    mpath = os.path.join('models', f'{r}', '0')\n",
    "    model = tf.keras.models.load_model(mpath)\n",
    "    acc = []\n",
    "    loss = []\n",
    "    for c in range(0, clients+1):\n",
    "        if c == 0:\n",
    "            metric = model.evaluate(datasets[f'{c}'], verbose=0)\n",
    "            global_loss.append(metric[0])\n",
    "            global_acc.append(metric[1])\n",
    "            print(f'round #{r} - Global: {metric}')\n",
    "        else:\n",
    "            weights = []\n",
    "            for weight in model.get_weights():\n",
    "                weights.append(weight*clients)\n",
    "            lpath = os.path.join('models', f'{r-1}', f'{c}')\n",
    "            lmodel = tf.keras.models.load_model(lpath)\n",
    "            for idx, weight in enumerate(lmodel.get_weights()):\n",
    "                weights[idx] = (weights[idx]-weight)/(clients-1)\n",
    "            model.set_weights(weights)\n",
    "            metric = model.evaluate(datasets[f'{c}'], verbose=0)\n",
    "            loss.append(metric[0])\n",
    "            acc.append(metric[1])\n",
    "    loss = sum(loss) / clients\n",
    "    acc = sum(acc) / clients\n",
    "    federated_loss.append(loss)\n",
    "    federated_acc.append(acc)\n",
    "    print(f'round #{r} - Federated: {(loss, acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e0e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLHub",
   "language": "python",
   "name": "flhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
